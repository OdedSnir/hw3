{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkqdwZcO97qw"
   },
   "source": [
    "# Exercise 3: MAP Classifier\n",
    "\n",
    "In this assignment you will implement Baysian learning\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This jupyter notebook contains all the step by step instructions needed for this part of the exercise.\n",
    "2. Write vectorized code whenever possible.\n",
    "3. You are responsible for the correctness of your code and should add as many tests as you see fit. Tests will not be graded nor checked.\n",
    "4. Write your functions in this notebook only.\n",
    "5. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/) and [numpy](https://www.numpy.org/devdocs/reference/) only. \n",
    "6. Your code must run without errors. During the environment setup, you were given a specific version of `numpy` to install. Changes of the configuration we provided are at your own risk. Code that cannot run will also earn you the grade of 0.\n",
    "7. Write your own code. Cheating will not be tolerated. \n",
    "8. Submission includes this notebook and the answers to the theoretical part. Answers to qualitative questions should be written in markdown cells (with $\\LaTeX$ support).\n",
    "9. You can add additional functions.\n",
    "10. Submission: zip only the completed jupyter notebook and the PDF with your solution for the theory part. Do not include the data or any directories. Name the file `ID1_ID2.zip` and submit **only one copy of the assignment**.\n",
    "\n",
    "## In this exercise you will perform the following:\n",
    "1. Uderstand Conditional Independence concept \n",
    "1. Implement density estimation using MLE\n",
    "1. Implement a Naive Bayes Classifier based on Uni-Normal distribution\n",
    "1. Implement a Full Bayes Classifier based on Multi-Normal distribution\n",
    "1. Implement a Discrete Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 596,
     "status": "ok",
     "timestamp": 1649263726391,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "S7n52AXs97q6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIlV22zUVJ7p"
   },
   "source": [
    "# Conditional independence  \n",
    "Define 3 random variables (RV) $X, Y, C$ s.t.:  \n",
    "1. $X, Y$ and $C$ are all binary\n",
    "2. The following conditions hold:  \n",
    "    * P(X=0) = 0.3\n",
    "    * P(Y=0) = 0.3\n",
    "    * P(C=0) = 0.5\n",
    "3. $X$ and $Y$ are not independent\n",
    "4. $X$ and $Y$ are conditionaly independent given $C$ $(X \\perp\\!\\!\\!\\perp Y |C)$\n",
    "\n",
    "In order to define those RV you need to fill the distributions (represent as python dictionaries) below\n",
    "and then write a function that prove that conditions 3 (`is_X_Y_depndendent`) and 4 (`is_X_Y_given_C_independent`) holds.\n",
    "\n",
    "Rcall that:   \n",
    "1. $P(X|Y) = \\frac{P(X, Y)}{P(Y)}$  \n",
    "2. $P(X, Y|C) = \\frac{P(X, Y, C)}{P(C)}$\n",
    "3. $(X \\perp\\!\\!\\!\\perp Y |C)$   iff  \n",
    "$\\forall x, y,c$: $p(X=x,Y=y|C=c)=p(X=x|C=c)p(Y=y|C=c)$  \n",
    "\n",
    "Make sure that all the probabilities are valid! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 614,
     "status": "ok",
     "timestamp": 1649263838473,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "-K57RiKSbKCi"
   },
   "outputs": [],
   "source": [
    "# You need to fill the None value with valid probabilities\n",
    "X = {0: 0.3, 1: 0.7}  # P(X=x)\n",
    "Y = {0: 0.3, 1: 0.7}  # P(Y=y)\n",
    "C = {0: 0.5, 1: 0.5}  # P(C=c)\n",
    "\n",
    "X_Y = { # are not independent so P(X, Y) != P(X)*P(Y) and P(X, Y) == P(X|Y)*P(Y)\n",
    "    (0, 0): 0.15, \n",
    "    (0, 1): 0.45,\n",
    "    (1, 0): 0.35,\n",
    "    (1, 1): 0.05\n",
    "}  # P(X=x, Y=y)\n",
    "\n",
    "X_C = { # model X and C as indepndent so P(X, C) == P(X)*P(C)\n",
    "    (0, 0): 0.15,\n",
    "    (0, 1): 0.15,\n",
    "    (1, 0): 0.35,\n",
    "    (1, 1): 0.35\n",
    "}  # P(X=x, C=y)\n",
    "\n",
    "Y_C = { # model Y and C as independent P(Y, C) == P(Y)*P(C)\n",
    "    (0, 0): 0.15,\n",
    "    (0, 1): 0.15,\n",
    "    (1, 0): 0.35,\n",
    "    (1, 1): 0.35\n",
    "}  # P(Y=y, C=c)\n",
    "\n",
    "X_Y_C = { # are conditionally indepndent so P(X, Y, C) = P(C)*P(X, Y|C) = P(C)*P(X|C)*P(Y|C) = C * (X_C/C) * (Y_C/C)\n",
    "    (0, 0, 0): C[0] * ( X_C[(0,0)] / C[0] ) * (Y_C[(0,0)] / C[0]),\n",
    "    (0, 0, 1): C[1] * ( X_C[(0,1)] / C[1] ) * (Y_C[(0,1)] / C[1]),\n",
    "    (0, 1, 0): C[0] * ( X_C[(0,0)] / C[0] ) * (Y_C[(1,0)] / C[0]),\n",
    "    (0, 1, 1): C[1] * ( X_C[(0,1)] / C[1] ) * (Y_C[(1,1)] / C[1]),\n",
    "    (1, 0, 0): C[0] * ( X_C[(1,0)] / C[0] ) * (Y_C[(0,0)] / C[0]),\n",
    "    (1, 0, 1): C[1] * ( X_C[(1,1)] / C[1] ) * (Y_C[(0,1)] / C[1]),\n",
    "    (1, 1, 0): C[0] * ( X_C[(1,0)] / C[0] ) * (Y_C[(1,0)] / C[0]),\n",
    "    (1, 1, 1): C[1] * ( X_C[(1,1)] / C[1] ) * (Y_C[(1,1)] / C[1])\n",
    "}  # P(X=x, Y=y, C=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.045"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1649263839191,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "QExLsNKrdegA"
   },
   "outputs": [],
   "source": [
    "# You may assume that X, Y and C have the same support (e.g. they are defined on the same space)\n",
    "# Note: since python suffer from numerical instability you may want to use np.isclose instead of the `==` operator\n",
    "def is_X_Y_depndendent(X, Y, X_Y):\n",
    "    \"\"\"\n",
    "    return True iff X and Y are depndendent\n",
    "    \"\"\"\n",
    "    \n",
    "    for x,y in X_Y:\n",
    "        val = X_Y[(x,y)]\n",
    "        independent_prob = X[x]*Y[y] \n",
    "        if not np.isclose(independent_prob, val):\n",
    "            return True\n",
    "\n",
    "    return False    \n",
    "\n",
    "\n",
    "def is_X_Y_given_C_independent(X, Y, C, X_C, Y_C, X_Y_C):\n",
    "    \"\"\"\n",
    "    return True iff X_given_C and Y_given_C are indepndendent\n",
    "\n",
    "    \"\"\"\n",
    "    for x, y, c in X_Y_C:\n",
    "        X_given_C = X_C[(x,c)]/C[c]\n",
    "        Y_given_C = Y_C[(y,c)]/C[c]\n",
    "        independent_prob = C[c] * X_given_C *Y_given_C\n",
    "        if not np.isclose(independent_prob, X_Y_C[x,y,c]):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are X and Y depenedent: True\n",
      "are X and Y conditionally indepnedent on C: True\n",
      "are X and C depenedent: False\n",
      "are Y and C depenedent: False\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "print(f\"are X and Y depenedent: {is_X_Y_depndendent(X, Y, X_Y)}\")\n",
    "print(f\"are X and Y conditionally indepnedent on C: {is_X_Y_given_C_independent(X, Y, C, X_C, Y_C, X_Y_C)}\")\n",
    "print(f\"are X and C depenedent: {is_X_Y_depndendent(X, C, X_C)}\")\n",
    "print(f\"are Y and C depenedent: {is_X_Y_depndendent(Y, C, Y_C)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZJBM6CCAyRB"
   },
   "source": [
    "# Maximum Likelihood estimation  \n",
    "\n",
    "In probability theory and statistics, the Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event.  \n",
    "The paramter of this distribution is the rate of events in that given time interval, annotated $\\lambda$  \n",
    "if $X$~$Pois(\\lambda)$  \n",
    "then $p(X=k|\\lambda) = \\frac{\\lambda^ke^{-\\lambda}}{k!}$  \n",
    "Where $X$ is a RV $\\lambda$ is the rate and $p$ is the pmf\n",
    "\n",
    "Implement the function `poisson_log_pmf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "ae0FUmlqFPA1"
   },
   "outputs": [],
   "source": [
    "def poisson_log_pmf(k, rate):\n",
    "    \"\"\"\n",
    "    k: A discrete instance\n",
    "    rate: poisson rate parameter (lambda)\n",
    "\n",
    "    return the log pmf value for instance k given the rate\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    prob = (rate**k)*(np.exp(-rate))/(np.math.factorial(k))\n",
    "    return np.log(prob)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for k=5, rate = 5 the log rate should be 1.754: 0.1754673697678507 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test \n",
    "print(f\"for k=5, rate = 5 the log rate should be 1.754: {poisson_log_pmf(5,5)} \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weRAiHJxIbuh"
   },
   "source": [
    "In the file poisson_1000_samples.csv there are 1000 points drawn from some poisson distribution with a fixed paramter $\\lambda$  \n",
    "\n",
    "In the following section you are going to find a rate that maximizes the likelihood function. You will do this in 2 diffrent ways:\n",
    "1. Iterative (`possion_iterative_mle`): given a list of possible rates (`rates`), calculate the log likelihood value for each rate and return the rate that has the maximum value\n",
    "2. Analytic (`possion_analytic_mle`): read the following blog: https://www.statology.org/mle-poisson-distribution/. This blog demonstrate how to derive the MLE of a poisson distribution. Understande the process and implement the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "executionInfo": {
     "elapsed": 311,
     "status": "error",
     "timestamp": 1649264049923,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "14_MylZP-15d",
    "outputId": "014abf33-e03c-4ef7-b787-eb1a00d0815b"
   },
   "outputs": [],
   "source": [
    "poisson_samples = pd.read_csv('data/poisson_1000_samples.csv').values.flatten()\n",
    "rates = np.linspace(1e-20, 20, num=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "executionInfo": {
     "elapsed": 295,
     "status": "ok",
     "timestamp": 1649264030874,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "Z8C_qIhKA9ZK"
   },
   "outputs": [],
   "source": [
    "def sum_of_log_possion_pmf(samples, rate): # returns the sum(samples after possion_log_pmf) == log(product(samples after regular possion_pmf))\n",
    "    current_pmf = lambda x: poisson_log_pmf(x, rate) \n",
    "    vfunc = np.vectorize(current_pmf)\n",
    "    log_prob_of_samples = vfunc(samples)\n",
    "    return np.sum(log_prob_of_samples)\n",
    "\n",
    "def get_poisson_log_likelihoods(samples, rates):\n",
    "    \"\"\"\n",
    "    samples: set of univariate discrete observations\n",
    "    rates: an iterable of rates to calculate log-likelihood by.\n",
    "\n",
    "    return: 1d numpy array, where each value represent that log-likelihood value of rates[i]\n",
    "    \"\"\"\n",
    "    return np.fromiter((sum_of_log_possion_pmf(samples, x) for x in rates), rates.dtype)\n",
    "    \n",
    "\n",
    "def possion_iterative_mle(samples, rates):\n",
    "    \"\"\"\n",
    "    samples: set of univariate discrete observations\n",
    "    rate: a rate to calculate log-likelihood by.\n",
    "\n",
    "    return: the rate that maximizes the likelihood \n",
    "    \"\"\"\n",
    "    rate = 0.\n",
    "    likelihoods = get_poisson_log_likelihoods(samples, rates)\n",
    "    # Your code goes here\n",
    "    \n",
    "    bestRateIndex = np.argmax(likelihoods)\n",
    "    rate = rates[bestRateIndex]\n",
    "    # End of your code\n",
    "    return rate\n",
    "\n",
    "def possion_analytic_mle(samples):\n",
    "    \"\"\"\n",
    "    samples: set of univariate discrete observations\n",
    "\n",
    "    return: the rate that maximizes the likelihood\n",
    "    \"\"\"\n",
    "    rate = (1/ samples.shape[0]) * np.sum(samples)\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "executionInfo": {
     "elapsed": 779,
     "status": "error",
     "timestamp": 1649264034100,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "i55OIUH1MIUh",
    "outputId": "b593730f-e29a-4ba0-fd56-0f2a8bc4f29d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = rates\n",
    "y = get_poisson_log_likelihoods(poisson_samples, rates)\n",
    "\n",
    "iterative_rate = possion_iterative_mle(poisson_samples, rates)\n",
    "analytic_rate = possion_analytic_mle(poisson_samples)\n",
    "plt.plot(x, y)\n",
    "plt.axvline(x=iterative_rate, linestyle=':', c='r', label=f\"iterative: {iterative_rate:.2f}\")\n",
    "plt.axvline(x=analytic_rate, linestyle='--', c='g', label=f\"analytic: {analytic_rate:.2f}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kL1Y_nmu97q9"
   },
   "source": [
    "\n",
    "# Normal Naive Bayes Classifier Vs Normal Full Bayes Classifier\n",
    "In the following section we are going to compare 2 models on a given dataset. <br>\n",
    "The 2 classifiers we are going to test are:\n",
    "1. Naive Bayes classifer.<br>\n",
    "1. Full Bayes classifier.<br>\n",
    "Recall that a Naive Bayes classifier makes the following assumption :<br> \n",
    "## $$ p(x_1, x_2, ..., x_n|A_j) = \\Pi p(x_i | A_j) $$\n",
    "But the full Bayes classifier will not make this assumption.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4Trl8uU97q-"
   },
   "source": [
    "### The Data Story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Z8oRyIG97rA"
   },
   "source": [
    "In a faraway land called **Randomistan** there is a rare animal called the **Randomammal**.<br> \n",
    "We have gathered data about this unique animal to help the **randomian** researchers in observing this beast. <br>\n",
    "For a 1000 days straight we have measured the temperature and the humidity in Randomistan and whether the Randomammal was spotted or not. <br>\n",
    "The well known randomian **Bob** is a bit of a lazy researcher so he likes to keep things simple, and so he assumes that the temperature and the humidity are independent given the class. <br>\n",
    "**Alice** on the other hand is a hard working researcher and does not make any assumptions, she's young and is trying to gain some fame in the randomian community.\n",
    "\n",
    "The dataset contains 2 features (**Temperature**, **Humidity**) alongside a binary label (**Spotted**) for each instance.<br>\n",
    "\n",
    "We are going to test 2 different classifiers :\n",
    "* Naive Bayes Classifier (Bob)\n",
    "* Full Bayes Classifier. (Alice)\n",
    "\n",
    "Both of our researchers assume that our features are normally distributed. But while Bob with his Naive classifier will assume that the features are independent, Alice and her Full Bayes classifier will not make this assumption.<br><br>\n",
    "Let's start off by loading the data (train, test) into a pandas dataframe and then converting them\n",
    "into numpy arrays.<br>\n",
    "The datafiles are :\n",
    "- randomammal_train.csv\n",
    "- randomammal_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wml9l2VJ97rB"
   },
   "outputs": [],
   "source": [
    "# Load the train and test set into a pandas dataframe and convert them into a numpy array.\n",
    "# The columns order: ['Temp', 'Humidity', 'Spotted']\n",
    "train_set = pd.read_csv('data/randomammal_train.csv').values\n",
    "test_set = pd.read_csv('data/randomammal_test.csv').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0Smujya97rD"
   },
   "source": [
    "# Data Visualization\n",
    "Draw a scatter plot of the training data where __x__=Temerature and **y**=Humidity. <br>\n",
    "Use color to distinguish points from different classes.<br>\n",
    "Stop for a minute to think about Alice and Bob's approaches and which one you expect to work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3GuXpOj97rF"
   },
   "outputs": [],
   "source": [
    "# Your code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtalzG-N97rG"
   },
   "source": [
    "## Bob's Naive Model\n",
    "\n",
    "Start with implementing the [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) probability density function in the next cell: \n",
    "$$ \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\cdot e ^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} $$\n",
    "Where :\n",
    "* $\\mu$ is the distribution mean.\n",
    "* $\\sigma$ is the distribution standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0MLP1cR_0d-"
   },
   "outputs": [],
   "source": [
    "def normal_pdf(x, mean, std):\n",
    "    \"\"\"\n",
    "    Calculate normal desnity function for a given x, mean and standrad deviation.\n",
    " \n",
    "    Input:\n",
    "    - x: A value we want to compute the distribution for.\n",
    "    - mean: The mean value of the distribution.\n",
    "    - std:  The standard deviation of the distribution.\n",
    " \n",
    "    Returns the normal distribution pdf according to the given mean and std for the given x.    \n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLQsBEja97rH"
   },
   "source": [
    "Implement the **NaiveNormalClassDistribution** in the next cell and build a distribution object for each class.\n",
    "Recall that when using the naive assumption, we assume our features are indepenent given the class. Meaning:\n",
    "$$ P(x_1, x_2 | Y) = p(x_1 | Y) \\cdot p(x_2 | Y)$$\n",
    "\n",
    "\n",
    "Since we assume our features are normally distributed we need to find the mean and std for each feature in order for us to compute those probabilites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSJRUYiZ97rI"
   },
   "outputs": [],
   "source": [
    "class NaiveNormalClassDistribution():\n",
    "    def __init__(self, dataset, class_value):\n",
    "        \"\"\"\n",
    "        A class which encapsulates the relevant parameters(mean, std) for a class conditinoal normal distribution.\n",
    "        The mean and std are computed from a given data set.\n",
    "        \n",
    "        Input\n",
    "        - dataset: The dataset as a 2d numpy array, assuming the class label is the last column\n",
    "        - class_value : The class to calculate the parameters for.\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        pass\n",
    "    \n",
    "    def get_prior(self):\n",
    "        \"\"\"\n",
    "        Returns the prior porbability of the class according to the dataset distribution.\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        pass\n",
    "    \n",
    "    def get_instance_likelihood(self, x):\n",
    "        \"\"\"\n",
    "        Returns the likelihhod porbability of the instance under the class according to the dataset distribution.\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        pass\n",
    "    \n",
    "    def get_instance_posterior(self, x):\n",
    "        \"\"\"\n",
    "        Returns the posterior porbability of the instance under the class according to the dataset distribution.\n",
    "        * Ignoring p(x)\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RDlv-Qu897rK"
   },
   "outputs": [],
   "source": [
    "# Build the a NaiveNormalClassDistribution for each class.\n",
    "naive_normal_CD_0 = NaiveNormalClassDistribution(train_set, 0)\n",
    "naive_normal_CD_1 = NaiveNormalClassDistribution(train_set, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3S8TaXg97rK"
   },
   "source": [
    "Implement the **MAPClassifier** class and build a MAPClassifier object containing the 2 distribution objects you just made above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fh-E75DE97rL"
   },
   "outputs": [],
   "source": [
    "class MAPClassifier():\n",
    "    def __init__(self, ccd0 , ccd1):\n",
    "        \"\"\"\n",
    "        A Maximum a posteriori classifier. \n",
    "        This class will hold 2 class distributions, one for class 0 and one for class 1, and will predict an instance\n",
    "        by the class that outputs the highest posterior probability for the given instance.\n",
    "    \n",
    "        Input\n",
    "            - ccd0 : An object contating the relevant parameters and methods for the distribution of class 0.\n",
    "            - ccd1 : An object contating the relevant parameters and methods for the distribution of class 1.\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        pass\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts the instance class using the 2 distribution objects given in the object constructor.\n",
    "    \n",
    "        Input\n",
    "            - An instance to predict.\n",
    "        Output\n",
    "            - 0 if the posterior probability of class 0 is higher and 1 otherwise.\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQfcw22U97rL"
   },
   "outputs": [],
   "source": [
    "naive_normal_classifier = MAPClassifier(naive_normal_CD_0, naive_normal_CD_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fK3PKXpf97rM"
   },
   "source": [
    "### Evaluate model\n",
    "Implement the **compute_accuracy** function in the next cell. Use it and the 2 distribution objects you created to compute the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eA39OpAp97rM"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(testset, map_classifier):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of a given a testset using a MAP classifier object.\n",
    "    \n",
    "    Input\n",
    "        - testset: The testset for which to compute the accuracy (Numpy array). where the class label is the last column\n",
    "        - map_classifier : A MAPClassifier object capable of prediciting the class for each instance in the testset.\n",
    "        \n",
    "    Ouput\n",
    "        - Accuracy = #Correctly Classified / #testset size\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1649236759557,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "X-p0Oo2A97rM",
    "outputId": "6ad81ff3-d37a-406a-83dd-0d061ffed43e"
   },
   "outputs": [],
   "source": [
    "# Compute the naive model accuracy and store it in the naive accuracy variable.\n",
    "naive_accuracy = compute_accuracy(test_set, naive_normal_classifier)\n",
    "naive_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_mGpmGM97rN"
   },
   "source": [
    "## Alice's Full Model\n",
    "\n",
    "Start with Implementing the [multivariate normal](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) distribution probability density function in the next cell.\n",
    "\n",
    "## $$ (2\\pi)^{-\\frac{d}{2}} det(\\Sigma )^{-\\frac{1}{2}} \\cdot e ^{-\\frac{1}{2}(x-\\mu)^T \\Sigma ^ {-1} (x - \\mu) }$$\n",
    "\n",
    "Where : \n",
    "* $\\mu$ is the distribution mean vector. (length 2 in our case)\n",
    "* $\\Sigma$ Is the distribution covarince matrix. (size 2x2 in our case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZTlpgi7Ojal"
   },
   "outputs": [],
   "source": [
    "def multi_normal_pdf(x, mean, cov):\n",
    "    \"\"\"\n",
    "    Calculate multi variable normal desnity function for a given x, mean and covarince matrix.\n",
    " \n",
    "    Input:\n",
    "    - x: A value we want to compute the distribution for.\n",
    "    - mean: The mean vector of the distribution.\n",
    "    - cov:  The covariance matrix of the distribution.\n",
    " \n",
    "    Returns the normal distribution pdf according to the given mean and var for the given x.    \n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIo8Ji8Z97rN"
   },
   "source": [
    "Implement the **MultiNormalClassDistribution** and build a distribution object for each class.\n",
    "\n",
    "In the full bayes model we will not make any simplyfing assumptions, meaning, we will use a multivariate normal distribution. <br>\n",
    "And so, we'll need to compute the mean of each feature and to compute the covariance between the features to build the covariance matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJDo4AeC97rO"
   },
   "outputs": [],
   "source": [
    "class MultiNormalClassDistribution():\n",
    "    def __init__(self, dataset, class_value):\n",
    "        \"\"\"\n",
    "        A class which encapsulate the relevant parameters(mean, cov matrix) for a class conditinoal multi normal distribution.\n",
    "        The mean and cov matrix (You can use np.cov for this!) will be computed from a given data set.\n",
    "        \n",
    "        Input\n",
    "        - dataset: The dataset as a numpy array\n",
    "        - class_value : The class to calculate the parameters for.\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        pass\n",
    "        \n",
    "    def get_prior(self):\n",
    "        \"\"\"\n",
    "        Returns the prior porbability of the class according to the dataset distribution.\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        pass\n",
    "    \n",
    "    def get_instance_likelihood(self, x):\n",
    "        \"\"\"\n",
    "        Returns the likelihood of the instance under the class according to the dataset distribution.\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        pass\n",
    "    \n",
    "    def get_instance_posterior(self, x):\n",
    "        \"\"\"\n",
    "        Returns the posterior porbability of the instance under the class according to the dataset distribution.\n",
    "        * Ignoring p(x)\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRxCmrfI97rO"
   },
   "outputs": [],
   "source": [
    "# Build the a MultiNormalClassDistribution for each class.\n",
    "multi_normal_CD_0 = MultiNormalClassDistribution(train_set, 0)\n",
    "multi_normal_CD_1 = MultiNormalClassDistribution(train_set, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMYfYFiF97rO"
   },
   "source": [
    "build a MAPClassifier object contating the 2 distribution objects you just made above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0dM1VNHn97rP"
   },
   "outputs": [],
   "source": [
    "multi_normal_classifier = MAPClassifier(multi_normal_CD_0, multi_normal_CD_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fmTzieM97rP"
   },
   "source": [
    "### Evaluate model\n",
    "Use the **compute_accuracy** function and the 2 distribution objects you created to compute the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 401,
     "status": "ok",
     "timestamp": 1649238247985,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "9Ihutafq97rP",
    "outputId": "9ba3c303-f2e1-44b5-f780-df7896cc5584"
   },
   "outputs": [],
   "source": [
    "# Compute the naive model accuracy and store it in the naive accuracy variable.\n",
    "full_accuracy = compute_accuracy(test_set, multi_normal_classifier)\n",
    "full_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_7u-ec397rQ"
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XxF0vm797rQ"
   },
   "source": [
    "Use a plot bar to showcase the models accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "executionInfo": {
     "elapsed": 393,
     "status": "ok",
     "timestamp": 1649238258489,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "tCi0JFha97rQ",
    "outputId": "d1a6baa6-9149-40e9-af8b-52caff47a5df"
   },
   "outputs": [],
   "source": [
    "# Bar plot of accuracy of each model side by side.\n",
    "plt.bar(x=['Naive', 'Full'], height=[naive_accuracy, full_accuracy])\n",
    "plt.title(\"Naive vs Full accuracy comparison\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUxx4QO697rR"
   },
   "source": [
    "# Comparing Max a posteriori, prior, and likelihood results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvoTqYI397rR"
   },
   "source": [
    "For each of the classifiers above (naive Bayes and full Bayes, in which we compare posterior probabilities), we explore how classifiers would perform if we compare (1) only prior probabilities or (2) only likelihoods. \n",
    "\n",
    "In this section, you will implement MaxPrior and MaxLikelihood classifiers similarly to MAPClassifier, and then visualize the performance of the three models (MAP, MaxPrior, and MaxLikelihood) for each of the examples of above (naive Bayes and full Bayes).\n",
    "\n",
    "For example, your visualization can be a graph where accuracy is the y-axis, \"MaxPrior\", \"MaxLikelihood\", and \"MAP\" are the x-axis values, and at each x-value, there will be two bars - one for the naive Bayes, and one for the full Bayes.  \n",
    "\n",
    "Other graphs (that make sense / are intuitive) will be accepted as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpIKZphj97rS"
   },
   "source": [
    "Implement the **MaxPrior** class and build a MaxPrior object like you did above with the **MAPClassifier**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2M5xSt4897rS"
   },
   "outputs": [],
   "source": [
    "class MaxPrior():\n",
    "    def __init__(self, ccd0 , ccd1):\n",
    "        \"\"\"\n",
    "        A Maximum prior classifier. \n",
    "        This class will hold 2 class distributions, one for class 0 and one for class 1, and will predicit an instance\n",
    "        by the class that outputs the highest prior probability for the given instance.\n",
    "    \n",
    "        Input\n",
    "            - ccd0 : An object contating the relevant parameters and methods for the distribution of class 0.\n",
    "            - ccd1 : An object contating the relevant parameters and methods for the distribution of class 1.\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        pass\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts the instance class using the 2 distribution objects given in the object constructor.\n",
    "    \n",
    "        Input\n",
    "            - An instance to predict.\n",
    "        Output\n",
    "            - 0 if the posterior probability of class 0 is higher and 1 otherwise.\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jOncByj97rS"
   },
   "source": [
    "Implement the **MaxLikelihood** class and build a MaxLikelihood object like you did above with the **MAPClassifier**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uk7culTl97rT"
   },
   "outputs": [],
   "source": [
    "class MaxLikelihood():\n",
    "    def __init__(self, ccd0 , ccd1):\n",
    "        \"\"\"\n",
    "        A Maximum Likelihood classifier. \n",
    "        This class will hold 2 class distributions, one for class 0 and one for class 1, and will predicit an instance\n",
    "        by the class that outputs the highest likelihood probability for the given instance.\n",
    "    \n",
    "        Input\n",
    "            - ccd0 : An object contating the relevant parameters and methods for the distribution of class 0.\n",
    "            - ccd1 : An object contating the relevant parameters and methods for the distribution of class 1.\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        pass\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts the instance class using the 2 distribution objects given in the object constructor.\n",
    "    \n",
    "        Input\n",
    "            - An instance to predict.\n",
    "        Output\n",
    "            - 0 if the posterior probability of class 0 is higher and 1 otherwise.\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2r0piw097rT"
   },
   "source": [
    "### Run and evaluate the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpKqIqTy97rU"
   },
   "source": [
    "Repeat the process you did for the MAPClassifier, now for the MaxPrior and MaxLikelihood classifiers:\n",
    "1. Feed the naive_normal distributions and the multi_normal distributions you made for each class into the new models you made in this section\n",
    "2. Evaluate the accuracies\n",
    "3. Plot the results as described in the beginning of this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7zQccmN97rV"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "####### YOUR CODE HERE ########\n",
    "# you may add cells as needed #\n",
    "###############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TWhBD4997rV"
   },
   "source": [
    "# Discrete Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKakgRD797rV"
   },
   "source": [
    "We will now build a discrete naive Bayes based classifier using **Laplace** smoothing.\n",
    "In the recitation, we saw how to compute the probability for each attribute value under each class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNv3VdNY97rW"
   },
   "source": [
    "$$ P(x_j | A_i) = \\frac{n_{ij} + 1}{n_i + |V_j|} $$\n",
    "Where:\n",
    "* $n_{ij}$ The number of training instances with the class $A_i$ and the value $x_j$ in the relevant attribute.\n",
    "* $n_i$ The number of training instances with the class $A_i$\n",
    "* $|V_j|$ The number of possible values of the relevant attribute.\n",
    "\n",
    "In order to compute the likelihood we assume:\n",
    "$$ P(x| A_i) = \\prod\\limits_{j=1}^{n}P(x_j|A_i) $$\n",
    "\n",
    "And to classify an instance we will choose : \n",
    "$$\\arg\\!\\max\\limits_{i} P(A_i) \\cdot P(x | A_i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95Y9WEKO97rW"
   },
   "source": [
    "## Data\n",
    "We will try to predict breast cancer again only this time from a different dataset, \n",
    "<br> you can read about the dataset here : [Breast Cancer Dataset](https://archive.ics.uci.edu/ml/datasets/breast+cancer)<br>\n",
    "Load the training set and test set provided for you in the data folder.\n",
    " - breast_trainset.csv\n",
    " - breast_testset.csv\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ldj_5a3d97rX"
   },
   "outputs": [],
   "source": [
    "# Load the train and test set into a pandas dataframe and convert them into a numpy array.\n",
    "train_set = pd.read_csv('data/breast_trainset.csv').values\n",
    "test_set = pd.read_csv('data/breast_testset.csv').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXzx4U0097rX"
   },
   "source": [
    "## Build A Discrete Naive Bayes Distribution for each class\n",
    "Implement the **DiscreteNBClassDistribution** in the next cell and build a distribution object for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hS-DkveU97rX"
   },
   "outputs": [],
   "source": [
    "EPSILLON = 1e-6 # if a certain value only occurs in the test set, the probability for that value will be EPSILLON.\n",
    "\n",
    "class DiscreteNBClassDistribution():\n",
    "    def __init__(self, dataset, class_value):\n",
    "        \"\"\"\n",
    "        A class which computes and encapsulate the relevant probabilites for a discrete naive bayes \n",
    "        distribution for a specific class. The probabilites are computed with laplace smoothing.\n",
    "        \n",
    "        Input\n",
    "        - dataset: The dataset as a numpy array.\n",
    "        - class_value: Compute the relevant parameters only for instances from the given class.\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        pass\n",
    "    \n",
    "    def get_prior(self):\n",
    "        \"\"\"\n",
    "        Returns the prior porbability of the class according to the dataset distribution.\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        pass\n",
    "    \n",
    "    def get_instance_likelihood(self, x):\n",
    "        \"\"\"\n",
    "        Returns the likelihood of the instance under the class according to the dataset distribution.\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        pass\n",
    "    \n",
    "    def get_instance_posterior(self, x):\n",
    "        \"\"\"\n",
    "        Returns the posterior porbability of the instance under the class according to the dataset distribution.\n",
    "        * Ignoring p(x)\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiJVXw5h97rY"
   },
   "outputs": [],
   "source": [
    "discrete_naive_CD_0 = DiscreteNBClassDistribution(train_set, 0)\n",
    "discrete_naive_CD_1 = DiscreteNBClassDistribution(train_set, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7Zfpdyt97rY"
   },
   "source": [
    "build a MAPClassifier object contating the 2 distribution objects you just made above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R46vXMqS97rZ"
   },
   "outputs": [],
   "source": [
    "discrete_naive_classifier = MAPClassifier(discrete_naive_CD_0, discrete_naive_CD_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKtJw1Ty97rZ"
   },
   "source": [
    "Use the **compute_accuracy** function and the 2 distribution objects you created to compute the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxwH3d4Z97rZ"
   },
   "outputs": [],
   "source": [
    "compute_accuracy(test_set, discrete_naive_classifier)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ProbabilisticModels.ipynb",
   "provenance": [
    {
     "file_id": "1bCEDw-NC2JWZstuBhGlo7VcB188Ft2K5",
     "timestamp": 1649263512426
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
